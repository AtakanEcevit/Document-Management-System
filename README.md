# Document Management System (DMS) with AI Chatbot

[![Angular](https://img.shields.io/badge/Frontend-Angular-red?logo=angular)](https://angular.dev/)
[![PrimeNG](https://img.shields.io/badge/UI-PrimeNG-blue?logo=primefaces)](https://primeng.org/)
[![FastAPI](https://img.shields.io/badge/Backend-FastAPI-teal?logo=fastapi)](https://fastapi.tiangolo.com/)
[![PostgreSQL](https://img.shields.io/badge/Database-PostgreSQL-blue?logo=postgresql)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Container-Docker-2496ED?logo=docker)](https://www.docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-brightgreen.svg)](https://github.com/AtakanEcevit/Document-Management-System/pulls)


---

## âœ¨ Highlights

* **Drag & drop** PDF uploads with automatic text extraction
* **AIâ€‘generated** summaries, keywords, and category tagging
* **Inline preview** with PDF.js
* **Chat with your documents** (contextâ€‘aware Q\&A)
* **FastAPI + PostgreSQL** backend, **Angular + PrimeNG** frontend
* **Dockerized** for easy setup and deployment

> \[!TIP]
> The chatbot remembers the **currently opened document** so you donâ€™t need to repeat file context.


---

## ğŸ§± Architecture

```
[Angular + PrimeNG]  â”€â”€â–º  [FastAPI]  â”€â”€â–º  [PostgreSQL]
         â”‚                        â”‚
   PDF.js preview           AI layer (LLM)
```

* **Frontend (Angular)**: Library view, search/filter, document detail, chat UI
* **Backend (FastAPI)**: Document ingestion, metadata & AI endpoints, REST API
* **Database (PostgreSQL)**: Documents, metadata (keywords/categories/summaries), chat history
* **AI Layer**: Pluggable LLM provider for summarization, keywording, and Q\&A

---

## ğŸ§° Tech Stack

| Layer    | Technology                                     |
| -------- | ---------------------------------------------- |
| Frontend | Angular (standalone), PrimeNG, PDF.js          |
| Backend  | FastAPI (Python)                               |
| Database | PostgreSQL (with connection pooling)           |
| AI Layer | LLM integration (summaries, keywords, chatbot) |
| Dev/Ops  | Docker / Docker Compose                        |

---


## ğŸš€ Quick Start

### Option A â€” Docker (recommended)

```bash
# in project root
cd deploy
docker-compose up --build
```

* Frontend: [http://localhost:8080](http://localhost:8080)
* Backend: [http://localhost:8000](http://localhost:8000)
* Database: localhost:5432

> \[!CAUTION]
> Stop any services already using **5432 (PostgreSQL)**, **4200 (frontend)**, or **8080 (backend)** before running Compose.

### Option B â€” Local Development

#### Prerequisites

* Node.js (LTS recommended)
* Python 3.10+
* PostgreSQL running locally

> \[!IMPORTANT]
> Start PostgreSQL **before** booting the backend, or the API wonâ€™t connect.

#### Backend

```bash
cd backend
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
# set your env vars (see below), then run:
uvicorn app.main:app --reload --port 8080
```

#### Frontend

```bash
cd frontend-angular
npm install
ng serve --port 4200
```

---

## âš™ï¸ Configuration

Create a `.env` file for the backend (example):

```env
# Database
DATABASE_URL=postgresql+psycopg://user:password@localhost:5432/dms

# AI Provider (set one or more depending on your implementation)
LLM_PROVIDER=openai            # or azure, anthropic, etc.
OPENAI_API_KEY=your_key_here   # if using OpenAI-compatible API

# App
BACKEND_CORS_ORIGINS=["http://localhost:4200"]
LOG_LEVEL=info
```

> \[!NOTE]
> Adjust variable names to match your codebase. If you support multiple providers, document them here.

If the frontend needs environment config, add `frontend/src/environments/*.ts` (Angular standard) and document the keys here (e.g., `apiBaseUrl`).

---

## ğŸ§  AI Features

* **Summaries**: Short overviews per document
* **Keywords & Categories**: Autoâ€‘generated tags for search & filtering
* **Chatbot**: Ask naturalâ€‘language questions about the open document; responses are contextâ€‘aware

> Implementation notes
>
> * Keep prompts and provider settings configurable via env vars.
> * Consider rateâ€‘limiting and caching for cost & latency control.
> * Log model + version for reproducibility.

---

## ğŸ–¥ï¸ Frontend (Angular)

* **Views**: Library (grid/list), Detail, Chat panel
* **UX**: Keyboard shortcuts â€” `/` to focus search, `Ctrl+S` to save
* **PrimeNG**: Tables, dialogs, toasts, inputs, file upload
* **PDF.js**: Inline preview with pagination & zoom

Development scripts (examples):

```bash
# dev
ng serve -o
# build
ng build --configuration production
```

---

## ğŸ” Security & Privacy

* Validate and sanitize uploads (size/type), store safely
* Restrict accessible origins via CORS
* Consider PII handling and data retention policies
* Plan authentication/authorization (see Roadmap)

---

## ğŸ› ï¸ Troubleshooting

> **Backend fails to start**: Check `DATABASE_URL`, ensure PostgreSQL is running and reachable.
>
> **CORS errors**: Confirm `BACKEND_CORS_ORIGINS` includes your frontend URL.
>
> **Large PDFs are slow**: Increase worker processes, chunk processing, or enable async streaming.
>
> **Port conflicts**: Stop services on 5432/4200/8080 or change exposed ports.

---

## ğŸ—ºï¸ Roadmap

* Multiâ€‘file chatbot support (ask across several PDFs)
* Export chats and notes
* User authentication & accounts (JWT/OAuth)
* Advanced NLP features (summarization modes, clustering)
* Usage analytics (optâ€‘in)
* Basic RBAC for shared workspaces

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome.

1. Fork the repo and create your branch: `feature/branch_name`
2. Commit with clear messages (consider Conventional Commits)
3. Open a PR and fill out the template
4. Link any related issues

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ™ Acknowledgements

* [FastAPI](https://fastapi.tiangolo.com/) & \[Pydantic]
* [Angular](https://angular.dev/) & [PrimeNG](https://primeng.org/)
* [PDF.js](https://mozilla.github.io/pdf.js/)

---

## ğŸ“– FAQ

**Q: Can I use another LLM provider?**
A: Yes â€” the AI layer is providerâ€‘agnostic as long as you implement the provider client and set the correct env vars.

**Q: How big can PDFs be?**
A: This depends on your server limits and parsing strategy. Consider a size cap and chunking for reliability.

**Q: Is authentication required?**
A: Not yet â€” itâ€™s on the roadmap. For production, enable auth and HTTPS.
